---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.5
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Set up working environment

```{python}

import pandas as pd
import json
import requests
from bs4 import BeautifulSoup as BS
import re
import warnings
import bokeh
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import codecs
from json import JSONDecoder
from functools import partial
pd.options.display.max_rows = 9999
pd.options.display.max_columns = 9999
warnings.filterwarnings("ignore")
```

# Subsetting Data


- keep user only who reviewed more than 100 times. this will eliminate budget constraint

```{python}
business = pd.read_json('data\\business.json', lines=True)
```

```{python}
business.shape
```

```{python}
business.tail(100)
```

```{python}
#business["categories"].value_counts()

```

```{python}
foods_lsit=["Chinese","Bagels","Pizza","Coffee & Tea","Mexican","Ice Cream & Frozen Yogurt","Italian",
"American (Traditional)","Sandwiches","Fast Food","Thai","American (New)","Burgers","Japanese","Vietnamese",
"Steakhouses","Bakeries","Sushi Bars","Juice Bars & Smoothies","Food Trucks","Hot Dogs","Chicken Wings","Donuts",
"Filipino","Korean","Mediterranean","Breakfast & Brunch","Delis","Seafood","Desserts","Sports Bars","Cafes","Ethiopian",
"Cocktail Bars","Indian","Kebab","Turkish","Arabic","Burgers","Barbeque","Vegan","Hookah Bars","Cupcakes","Soup",
"Asian","Cheesesteaks","Asian Fusion","Persian/Iranian","Greek","French","German","Brazilian","Seafood"]


```

```{python}
pattern = '|'.join(foods_lsit)
pattern
```

```{python}
business["include"]=business.categories.str.contains(pattern)
```

```{python}
business=business.loc[(business['include']==True),:]
```

```{python}
business=business.loc[(business['city']=='Las Vegas') ,:]
```

```{python}
business=business.loc[(business['review_count']>=40) ,:]
business_to_include=business[["business_id"]]
business_to_include.head()
```

```{python}

business.shape
```

```{python}
for index, row in business.iterrows():
    for food in foods_lsit:
        if food in row["categories"].split():
            business.loc[index,'foods']=food
            break
    
```

```{python}
del business["categories"]
```

```{python}
business.rename(columns={"foods":'categories'},inplace=True)
```

```{python}
business = business.dropna(axis=0, subset=['categories'])
```

# subset datasets 


## review cleaning

```{python}
review=pd.read_pickle("data\\review.pkl")
```

```{python}
review.shape
```

```{python}
review=pd.merge(review,business_to_include,how='inner',on='business_id')
```

```{python}
review['yeni']=1

abc=review.groupby(['user_id']).sum()

abc=abc.loc[abc['yeni']>=40,:]

abc=abc.reset_index()
users_to_include=abc[["user_id"]]
users_to_include.head()

review=pd.merge(review,users_to_include,how='inner',on='user_id')
```

```{python}
review.tail(5)
```

```{python}
review.shape
```

```{python}
review['yeni']=1

abc=review.groupby(['business_id']).sum()

abc=abc.loc[abc['yeni']>=40,:]

abc=abc.reset_index()
business_to_include=abc[["business_id"]]
business_to_include.head()
```

```{python}
review=pd.merge(review,business_to_include,how='inner',on='business_id')
```

```{python}
#review["business_id"].value_counts()
```

```{python}
# review['stars']=review['stars'].astype(int)
# review['useful']=review['useful'].astype(float)
# review.info()

# review=review.loc[review['useful']>=20,:]
```

```{python}
review=review.loc[:,["business_id","stars","user_id"]]

```

# Load data set and merge them

```{python}
business=business[["business_id","name","categories"]]

```

```{python}
business=business.sort_values("business_id")
```

```{python}
business.shape
```

```{python}
business["movieId"] = business.index + 1
```

```{python}
business.head()
```

```{python}
business.rename(columns={'name':'title','categories':'genres'},inplace=True)
```

```{python}
business_id_codes=business[['business_id','movieId']]
business_id_codes.head()
```

```{python}
business=business.sort_values("movieId")
```

```{python}
business=business[["movieId","title","genres"]]
business.set_index("movieId",inplace=True)
```

```{python}
business_id_codes.shape
```

```{python}
business.head()
```

```{python}
business.to_csv("data\\movies.csv")
```

```{python}
review.head()
```

```{python}
ratings=pd.merge(business_id_codes,review,on="business_id",how="inner")
```

```{python}
ratings=ratings.loc[:,["user_id","business_id","stars"]]
```

```{python}
ratings.head()
```

```{python}
ratings.shape
```

```{python}
user_id_code=ratings.drop_duplicates(subset=["user_id"], keep="first")
```

```{python}
user_id_code.shape
```

```{python}
user_id_code.reset_index(inplace=True)
```

```{python}
user_id_code["userId"]=user_id_code.index + 1
```

```{python}
user_id_code=user_id_code[["user_id","userId"]]
```

```{python}
user_id_code.head()
```

```{python}
ratings=pd.merge(user_id_code,ratings,on='user_id',how='inner')
```

```{python}
ratings=pd.merge(ratings,business_id_codes,on='business_id',how='inner')
```

```{python}
ratings=ratings[["userId","movieId","stars"]]
```

```{python}
ratings["timestamp"]=1260759144
```

```{python}
ratings.rename(columns={'stars':'rating'},inplace=True)
```

```{python}
ratings['yeni']=1
```

```{python}
abc=ratings.groupby(['userId']).sum()
```

```{python}
abc=abc.loc[abc['yeni']>=40,:]
```

```{python}
abc=abc.reset_index()
```

```{python}
users_to_include=abc[["userId"]]
```

```{python}
users_to_include.head()
```

```{python}
ratings=pd.merge(ratings,users_to_include,how='inner',on='userId')
```

```{python}
ratings.set_index("userId",inplace=True)
```

```{python}
ratings.head()
```

```{python}
ratings.shape
```

```{python}
ratings.to_csv("data\\ratings.csv")
```

```{python}

```

```{python}

```

```{python}

```

```{python}

```
