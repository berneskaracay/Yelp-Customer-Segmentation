---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.5
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Set up working environment

```{python}

import pandas as pd
import json
import requests
from bs4 import BeautifulSoup as BS
import re
import warnings
import bokeh
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import codecs
from json import JSONDecoder
from functools import partial
pd.options.display.max_rows = 999
pd.options.display.max_columns = 999
warnings.filterwarnings("ignore")
```

# Get data and save as pickle data types

```{python}
user = pd.read_json('data\\user.json', lines=True)
user.head()
```

```{python}
user[['review_count']].mean()
```

```{python}
user['useful']=user['useful'].astype(float)
user['fans']=user['fans'].astype(float)
user['review_count']=user['review_count'].astype(float)


#user=user.loc[user['useful']>40,:]
#user=user.loc[user['fans']>5,:]
user=user.loc[user['review_count']>50,:]
```

```{python}
user.shape
```

```{python}
user.to_pickle("data\\user.pkl")
user.head()
```

```{python}
# checkin = pd.read_json('data\\checkin.json', lines=True)
# checkin.to_pickle("data\\checkin.pkl")
# checkin.head()
# review = pd.read_json('data\\review.json', lines=True)
# review.to_pickle("data\\review.pkl")
```

```{python}
# tip = pd.read_json('data\\tip.json', lines=True)
# tip.to_pickle("data\\tip.pkl")
# tip.head()
```

```{python}
business = pd.read_json('data\\business.json', lines=True)
```

```{python}
business.shape
```

```{python}
business.head()
```

```{python}
#business["review_count"].value_counts()
```

```{python}

business=business.loc[(business['city']=='Las Vegas') ,:]
#business=business.loc[business['review_count']>20,:]
```

```{python}
#business.to_pickle("data\\business.pkl")
business.to_pickle("data\\business1.pkl")
business.shape
```

```{python}
# photo = pd.read_json('data\\photo.json', lines=True)
# photo.to_pickle("data\\photo.pkl")
# photo.head()
```

# subset datasets 


## review cleaning

```{python}
review=pd.read_pickle("data\\review.pkl")
```

```{python}
review.shape
```

```{python}
review['yeni']=1

abc=review.groupby(['user_id']).sum()

abc=abc.loc[abc['yeni']>=40,:]

abc=abc.reset_index()
users_to_include=abc[["user_id"]]
users_to_include.head()

review=pd.merge(review,users_to_include,how='inner',on='user_id')
```

```{python}
review.tail(30)
```

```{python}
review.shape
```

```{python}
# review['stars']=review['stars'].astype(int)
# review['useful']=review['useful'].astype(float)
# review.info()

# review=review.loc[review['useful']>=20,:]
```

```{python}
review=review.loc[:,["business_id","stars","user_id"]]
review.shape

review.head()

review.to_pickle("data\\review1")
```

## Business data slicing

```{python}
# business=pd.read_pickle('data\\business.pkl')

# business.head()

# business['city'].value_counts()

# business=business.loc[(business['city']=='Las Vegas') & (business['review_count']>100),:]

# business=business.loc[:,["categories","business_id","name"]]

# business.to_pickle("data\\business1")
```

## user slicing

```{python}
# user=pd.read_pickle('data\\user.pkl')

# user.head()

# user.head()

# user=user.loc[:,['user_id','review_count','useful']]
# user.head()

# user.info()

# user=user.loc[(user['review_count']>60) & (user['useful']>10),:]
# user.shape

# user.to_pickle('data\\user1')
```

# Load data set and merge them

```{python}
user1=pd.read_pickle('data\\user.pkl')
user1.shape
```

```{python}
user1.head()
```

```{python}
review1=pd.read_pickle('data\\review1')
review1.shape
```

```{python}
review1.head()
```

```{python}
business1=pd.read_pickle('data\\business1')
business1.reset_index(inplace=True)

business1=business1.drop(["index"], axis=1)
business1.shape
```

```{python}
business1.head()
```

```{python}
business1=business1[["business_id","name","categories"]]
business1=business1.fillna(0)
```

```{python}
business1=business1.sort_values("business_id")
```

```{python}
business1.shape
```

```{python}
business1["movieId"] = business1.index + 1
```

```{python}
business1.head()
```

```{python}
business1.rename(columns={'name':'title','categories':'genres'},inplace=True)
```

```{python}
business_id_codes=business1[['business_id','movieId']]
business_id_codes.head()
```

```{python}
business1=business1.sort_values("movieId")
```

```{python}
business1=business1.drop(["business_id"], axis=1)
```

```{python}
business1=business1[["movieId","title","genres"]]
business1.set_index("movieId",inplace=True)
```

```{python}
business1.shape
```

```{python}
business1.head()
```

```{python}
business1.to_csv("data\\movies.csv")
```

```{python}
review1=pd.read_pickle('data\\review1')
review1.shape
```

```{python}
restaurant=pd.read_pickle('data\\business1')
restaurant.shape
```

```{python}
restaurant.head()
```

```{python}
ratings=pd.merge(restaurant,review1,on="business_id",how="inner")
```

```{python}
ratings=ratings.loc[:,["user_id","business_id","stars"]]
```

```{python}
ratings.head()
```

```{python}
ratings.shape
```

```{python}
user_id_code=ratings.drop_duplicates(subset=["user_id"], keep="first")
```

```{python}
user_id_code.shape
```

```{python}
user_id_code.reset_index(inplace=True)
```

```{python}
user_id_code["userId"]=user_id_code.index + 1
```

```{python}
user_id_code=user_id_code[["user_id","userId"]]
```

```{python}
user_id_code.head()
```

```{python}
ratings=pd.merge(user_id_code,ratings,on='user_id',how='inner')
```

```{python}
ratings=pd.merge(ratings,business_id_codes,on='business_id',how='inner')
```

```{python}
ratings=ratings[["userId","movieId","stars"]]
```

```{python}
ratings["timestamp"]=1260759144
```

```{python}
ratings.rename(columns={'stars':'rating'},inplace=True)
```

```{python}
ratings['yeni']=1
```

```{python}
abc=ratings.groupby(['userId']).sum()
```

```{python}
abc=abc.loc[abc['yeni']>=40,:]
```

```{python}
abc=abc.reset_index()
```

```{python}
users_to_include=abc[["userId"]]
```

```{python}
users_to_include.head()
```

```{python}
ratings=pd.merge(ratings,users_to_include,how='inner',on='userId')
```

```{python}
ratings.set_index("userId",inplace=True)
```

```{python}
ratings.head()
```

```{python}
ratings.to_csv("data\\ratings.csv")
```

```{python}

```

```{python}

```

```{python}

```
