---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.5
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Set up working environment

```{python}

import pandas as pd
import json
import requests
from bs4 import BeautifulSoup as BS
import re
import warnings
import bokeh
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import codecs
from json import JSONDecoder
from functools import partial
pd.options.display.max_rows = 999
pd.options.display.max_columns = 999
warnings.filterwarnings("ignore")
```

# Subsetting Data


- keep user only who reviewed more than 100 times. this will eliminate budget constraint

```{python}
user=pd.read_pickle("data\\user.pkl")
user.shape
```

```{python}
user['useful']=user['useful'].astype(float)
user['fans']=user['fans'].astype(float)
user['review_count']=user['review_count'].astype(float)


user=user.loc[user['review_count']>100,:]
```

```{python}
user.shape
```

```{python}
business = pd.read_json('data\\business.json', lines=True)
```

```{python}
business.shape
```

```{python}
business.head()
```

```{python}
business=business.loc[(business['city']=='Las Vegas') ,:]
business=business.loc[(business['review_count']>=300) ,:]
```

```{python}

business.shape
```

# subset datasets 


## review cleaning

```{python}
review=pd.read_pickle("data\\review.pkl")
```

```{python}
review.shape
```

```{python}
review['yeni']=1

abc=review.groupby(['user_id']).sum()

abc=abc.loc[abc['yeni']>=100,:]

abc=abc.reset_index()
users_to_include=abc[["user_id"]]
users_to_include.head()

review=pd.merge(review,users_to_include,how='inner',on='user_id')
```

```{python}
review.tail(30)
```

```{python}
review.shape
```

```{python}
# review['stars']=review['stars'].astype(int)
# review['useful']=review['useful'].astype(float)
# review.info()

# review=review.loc[review['useful']>=20,:]
```

```{python}
review=review.loc[:,["business_id","stars","user_id"]]

```

# Load data set and merge them

```{python}
business=business[["business_id","name","categories"]]

```

```{python}
business=business.sort_values("business_id")
```

```{python}
business.shape
```

```{python}
business["movieId"] = business.index + 1
```

```{python}
business.head()
```

```{python}
business.rename(columns={'name':'title','categories':'genres'},inplace=True)
```

```{python}
business_id_codes=business[['business_id','movieId']]
business_id_codes.head()
```

```{python}
business=business.sort_values("movieId")
```

```{python}
business=business[["movieId","title","genres"]]
business.set_index("movieId",inplace=True)
```

```{python}
business_id_codes.shape
```

```{python}
business.head()
```

```{python}
business.to_csv("data\\movies.csv")
```

```{python}
review.head()
```

```{python}
ratings=pd.merge(business_id_codes,review,on="business_id",how="inner")
```

```{python}
ratings=ratings.loc[:,["user_id","business_id","stars"]]
```

```{python}
ratings.head()
```

```{python}
ratings.shape
```

```{python}
user_id_code=ratings.drop_duplicates(subset=["user_id"], keep="first")
```

```{python}
user_id_code.shape
```

```{python}
user_id_code.reset_index(inplace=True)
```

```{python}
user_id_code["userId"]=user_id_code.index + 1
```

```{python}
user_id_code=user_id_code[["user_id","userId"]]
```

```{python}
user_id_code.head()
```

```{python}
ratings=pd.merge(user_id_code,ratings,on='user_id',how='inner')
```

```{python}
ratings=pd.merge(ratings,business_id_codes,on='business_id',how='inner')
```

```{python}
ratings=ratings[["userId","movieId","stars"]]
```

```{python}
ratings["timestamp"]=1260759144
```

```{python}
ratings.rename(columns={'stars':'rating'},inplace=True)
```

```{python}
ratings['yeni']=1
```

```{python}
abc=ratings.groupby(['userId']).sum()
```

```{python}
abc=abc.loc[abc['yeni']>=40,:]
```

```{python}
abc=abc.reset_index()
```

```{python}
users_to_include=abc[["userId"]]
```

```{python}
users_to_include.head()
```

```{python}
ratings=pd.merge(ratings,users_to_include,how='inner',on='userId')
```

```{python}
ratings.set_index("userId",inplace=True)
```

```{python}
ratings.head()
```

```{python}
ratings.to_csv("data\\ratings.csv")
```

```{python}

```

```{python}

```

```{python}

```
